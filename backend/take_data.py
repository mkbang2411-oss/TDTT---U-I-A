from serpapi import GoogleSearch
import pandas as pd
import os
import time
from requests.exceptions import ChunkedEncodingError
import requests

# ‚öôÔ∏è C·∫•u h√¨nh
SERP_API_KEY = "a3ce5e1007e887b80f0c3114d9bd93854917de1e7caae81e7887148f233072a4"
CSV_FILE = "Data.csv"


def get_places(query: str, lat: float, lon: float, retries=3, wait=5):
    """G·ªçi SerpAPI v·ªõi retry khi g·∫∑p l·ªói ChunkedEncoding"""
    if not SERP_API_KEY:
        print("‚ö†Ô∏è Ch∆∞a c√≥ SERP_API_KEY.")
        return []

    params = {
        "engine": "google_maps",
        "q": query,
        "ll": f"@{lat},{lon},15z",
        "type": "search",
        "hl": "vi",
        "api_key": SERP_API_KEY
    }

    for attempt in range(retries):
        try:
            search = GoogleSearch(params)
            results = search.get_dict()
            return results.get("local_results", [])
        except (ChunkedEncodingError, requests.exceptions.RequestException) as e:
            print(f"‚ö†Ô∏è L·ªói khi crawl ({lat}, {lon}): {e}")
            if attempt < retries - 1:
                print(f"‚è≥ Th·ª≠ l·∫°i sau {wait} gi√¢y...")
                time.sleep(wait)
            else:
                print("‚ùå B·ªè qua qu·∫≠n/huy·ªán n√†y.")
                return []


def parse_place_data(places: list):
    """Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ SerpAPI th√†nh DataFrame chu·∫©n c√°c c·ªôt"""
    if not places:
        return pd.DataFrame()

    records = []
    for p in places:
        if "gps_coordinates" not in p:
            continue

        menu_items = ", ".join([i.get("title", "") for i in p.get("menu_items", [])]) if "menu_items" in p else ""
        price = p.get("price", p.get("price_level", ""))
        gio_mo_cua = p.get("open_state") or p.get("hours") or "Kh√¥ng r√µ gi·ªù m·ªü c·ª≠a"

        records.append({
            "ten_quan": p.get("title", ""),
            "dia_chi": p.get("address", ""),
            "so_dien_thoai": p.get("phone", ""),
            "rating": p.get("rating", ""),
            "gio_mo_cua": gio_mo_cua,
            "lat": p["gps_coordinates"]["latitude"],
            "lon": p["gps_coordinates"]["longitude"],
            "gia_trung_binh": price,
            "thuc_don": menu_items,
            "hinh_anh": "",
            "data_id": p.get("data_id", ""),
            "khau_vi": "",
            "mo_ta": ""
        })

    df = pd.DataFrame(records)
    return df[[
        "ten_quan", "dia_chi", "so_dien_thoai", "rating", "gio_mo_cua",
        "lat", "lon", "gia_trung_binh", "thuc_don", "hinh_anh", "data_id",
        "khau_vi", "mo_ta"
    ]]


def save_places_to_csv(df_new: pd.DataFrame, CSV_FILE: str = CSV_FILE):
    """L∆∞u DataFrame v√†o CSV, tr√°nh tr√πng l·∫∑p"""
    if df_new.empty:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi.")
        return

    folder = os.path.dirname(CSV_FILE)
    if folder:
        os.makedirs(folder, exist_ok=True)

    if not os.path.exists(CSV_FILE) or os.stat(CSV_FILE).st_size == 0:
        df_new.to_csv(CSV_FILE, index=False)
        print(f"üíæ T·∫°o m·ªõi {CSV_FILE} ({len(df_new)} d√≤ng).")
        return

    try:
        df_old = pd.read_csv(CSV_FILE)
    except Exception:
        df_old = pd.DataFrame()

    df_all = pd.concat([df_old, df_new], ignore_index=True)
    df_all.drop_duplicates(subset=["ten_quan", "dia_chi"], inplace=True)
    df_all.to_csv(CSV_FILE, index=False)
    print(f"‚úÖ C·∫≠p nh·∫≠t {CSV_FILE}: t·ªïng {len(df_all)} qu√°n.")


def crawl_and_save_places(query: str, lat: float, lon: float):
    """Crawl d·ªØ li·ªáu + parse + l∆∞u CSV"""
    print(f"üöÄ Crawling '{query}' t·∫°i ({lat}, {lon}) ...")
    places = get_places(query, lat, lon)
    df_new = parse_place_data(places)
    if not df_new.empty:
        save_places_to_csv(df_new, CSV_FILE)
    else:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu thu ƒë∆∞·ª£c.")
    return df_new.to_dict(orient="records")


if __name__ == "__main__":
    # T·ªça ƒë·ªô trung t√¢m qu·∫≠n/huy·ªán TP.HCM
    DISTRICTS = {
        "Qu·∫≠n 1": (10.77566, 106.70042),
        "Qu·∫≠n 3": (10.78353, 106.68710),
        "Qu·∫≠n 4": (10.76073, 106.70755),
        "Qu·∫≠n 5": (10.75669, 106.66370),
        "Qu·∫≠n 6": (10.74805, 106.63550),
        "Qu·∫≠n 7": (10.73861, 106.72639),
        "Qu·∫≠n 8": (10.72464, 106.62863),
        "Qu·∫≠n 10": (10.77347, 106.66700),
        "Qu·∫≠n 11": (10.76287, 106.65015),
        "Qu·∫≠n 12": (10.86752, 106.64113),
        "B√¨nh Th·∫°nh": (10.81058, 106.70915),
        "G√≤ V·∫•p": (10.83806, 106.66750),
        "Ph√∫ Nhu·∫≠n": (10.79919, 106.68026),
        "T√¢n B√¨nh": (10.80203, 106.64931),
        "T√¢n Ph√∫": (10.78640, 106.62883),
        "Th√†nh ph·ªë Th·ªß ƒê·ª©c": (10.84941, 106.75371)
    }

    query = input("üîç Nh·∫≠p t·ª´ kh√≥a mu·ªën t√¨m (vd: ph·ªü, tr√† s·ªØa, c∆°m t·∫•m): ").strip()
    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl '{query}' to√†n TP.HCM ...\n")

    for district, (lat, lon) in DISTRICTS.items():
        print(f"üìç {district}: Crawling t·∫°i t√¢m qu·∫≠n/huy·ªán ...")
        crawl_and_save_places(query=query, lat=lat, lon=lon)
        time.sleep(1)  # tr√°nh spam API

    print("üéâ Ho√†n t·∫•t crawl to√†n TP.HCM! D·ªØ li·ªáu l∆∞u trong Data.csv")
