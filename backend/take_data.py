from serpapi import GoogleSearch
import pandas as pd
import os
import time

# ‚öôÔ∏è C·∫•u h√¨nh
SERP_API_KEY = "919519991034d358c7da2ae6f11bc21ded6a8e50a6193c568000e4ef8c9d8e2a"  # Nh·ªõ ƒëi·ªÅn key th·∫≠t c·ªßa b·∫°n
CSV_FILE = "Data.csv"


def get_places(query: str, lat: float, lon: float):
    """G·ªçi SerpAPI ƒë·ªÉ l·∫•y danh s√°ch qu√°n g·∫ßn v·ªã tr√≠ ch·ªâ ƒë·ªãnh."""
    if not SERP_API_KEY:
        print("‚ö†Ô∏è Ch∆∞a c√≥ SERP_API_KEY. H√£y ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c s·ª≠a trong code.")
        return []

    params = {
        "engine": "google_maps",
        "q": query,
        "ll": f"@{lat},{lon},15z",
        "type": "search",
        "hl": "vi",
        "api_key": SERP_API_KEY
    }

    search = GoogleSearch(params)
    results = search.get_dict()
    return results.get("local_results", [])


def parse_place_data(places: list):
    """Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ SerpAPI th√†nh DataFrame chu·∫©n c√°c c·ªôt"""
    if not places:
        return pd.DataFrame()

    records = []
    for p in places:
        if "gps_coordinates" not in p:
            continue

        # Danh s√°ch m√≥n
        menu_items = ""
        if "menu_items" in p and isinstance(p["menu_items"], list):
            menu_items = ", ".join([i.get("title", "") for i in p["menu_items"]])

        # Gi√° trung b√¨nh
        price = p.get("price", p.get("price_level", ""))

        # Gi·ªù m·ªü c·ª≠a
        gio_mo_cua = p.get("open_state") or p.get("hours") or "Kh√¥ng r√µ gi·ªù m·ªü c·ª≠a"

        # Th√™m c√°c c·ªôt m·ªõi: kh·∫©u v·ªã, m√¥ t·∫£ (n·∫øu SerpAPI kh√¥ng c√≥, ƒë·ªÉ tr·ªëng)
        khau_vi = p.get("keywords", "")  # v√≠ d·ª• SerpAPI tr·∫£ v·ªÅ tags/keywords
        mo_ta = p.get("description", "")

        records.append({
            "ten_quan": p.get("title", ""),
            "dia_chi": p.get("address", ""),
            "so_dien_thoai": p.get("phone", ""),
            "rating": p.get("rating", ""),
            "gio_mo_cua": gio_mo_cua,
            "lat": p["gps_coordinates"]["latitude"],
            "lon": p["gps_coordinates"]["longitude"],
            "gia_trung_binh": price,
            "thuc_don": menu_items,
            "hinh_anh": "",  # b·∫°n c√≥ th·ªÉ ƒëi·ªÅn link h√¨nh n·∫øu mu·ªën
            "data_id": p.get("data_id", ""),
            "khau_vi": "",
            "mo_ta": ""
        })

    # Ch·ªçn th·ª© t·ª± c·ªôt chu·∫©n
    df = pd.DataFrame(records)
    df = df[[
        "ten_quan", "dia_chi", "so_dien_thoai", "rating", "gio_mo_cua",
        "lat", "lon", "gia_trung_binh", "thuc_don", "hinh_anh", "data_id",
        "khau_vi", "mo_ta"
    ]]
    return df



def save_places_to_csv(df_new: pd.DataFrame, CSV_FILE: str = CSV_FILE):
    """L∆∞u DataFrame v√†o CSV, tr√°nh tr√πng l·∫∑p"""
    if df_new.empty:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi.")
        return

    folder = os.path.dirname(CSV_FILE)
    if folder:
        os.makedirs(folder, exist_ok=True)

    if not os.path.exists(CSV_FILE) or os.stat(CSV_FILE).st_size == 0:
        df_new.to_csv(CSV_FILE, index=False)
        print(f"üíæ T·∫°o m·ªõi {CSV_FILE} ({len(df_new)} d√≤ng).")
        return

    try:
        df_old = pd.read_csv(CSV_FILE)
    except Exception:
        df_old = pd.DataFrame()

    df_all = pd.concat([df_old, df_new], ignore_index=True)
    df_all.drop_duplicates(subset=["ten_quan", "dia_chi"], inplace=True)
    df_all.to_csv(CSV_FILE, index=False)
    print(f"‚úÖ C·∫≠p nh·∫≠t {CSV_FILE}: t·ªïng {len(df_all)} qu√°n.")


def crawl_and_save_places(query: str, lat: float, lon: float):
    """Crawl d·ªØ li·ªáu + parse + l∆∞u CSV"""
    print(f"üöÄ Crawling '{query}' t·∫°i ({lat}, {lon}) ...")
    places = get_places(query, lat, lon)
    df_new = parse_place_data(places)
    if not df_new.empty:
        save_places_to_csv(df_new, CSV_FILE)
    else:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu thu ƒë∆∞·ª£c.")
    return df_new.to_dict(orient="records")


# ‚úÖ Cho ph√©p ch·∫°y th·ªß c√¥ng ƒë·ªÉ test CLI
if __name__ == "__main__":
    DISTRICTS = {
         "Qu·∫≠n 1": (10.77566, 106.70042),
    "Qu·∫≠n 3": (10.78353, 106.68710),
    "Qu·∫≠n 4": (10.76073, 106.70755),
    "Qu·∫≠n 5": (10.75669, 106.66370),
    "Qu·∫≠n 6": (10.74805, 106.63550),
    "Qu·∫≠n 7": (10.73861, 106.72639),
    "Qu·∫≠n 8": (10.72464, 106.62863),
    "Qu·∫≠n 10": (10.77347, 106.66700),
    "Qu·∫≠n 11": (10.76287, 106.65015),
    "Qu·∫≠n 12": (10.86752, 106.64113),
    "B√¨nh Th·∫°nh": (10.81058, 106.70915),
    "G√≤ V·∫•p": (10.83806, 106.66750),
    "Ph√∫ Nhu·∫≠n": (10.79919, 106.68026),
    "T√¢n B√¨nh": (10.80203, 106.64931),
    "T√¢n Ph√∫": (10.78640, 106.62883),

    # Th√†nh ph·ªë Th·ªß ƒê·ª©c (t∆∞∆°ng ·ª©ng Qu·∫≠n 2 + 9 + Th·ªß ƒê·ª©c c≈©)
    "Th√†nh ph·ªë Th·ªß ƒê·ª©c": (10.84941, 106.75371)
    }

    query = input("üîç Nh·∫≠p t·ª´ kh√≥a mu·ªën t√¨m (vd: ph·ªü, tr√† s·ªØa, c∆°m t·∫•m): ").strip()
    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl '{query}' ...\n")

    for name, (lat, lon) in DISTRICTS.items():
        print(f"üìç {name} ({lat}, {lon})")
        data = crawl_and_save_places(query, lat, lon)
        print(f"‚úÖ {name}: {len(data)} k·∫øt qu·∫£.\n")
        time.sleep(5)

    print("üéâ Ho√†n t·∫•t! D·ªØ li·ªáu l∆∞u trong Data.csv")