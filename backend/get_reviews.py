import pandas as pd
import requests
import json
import os
import time

# ‚öôÔ∏è C·∫•u h√¨nh
SERP_API_KEY = "c56afb87e174c9e1d73c0687e87900bfe2801e203aaba618732e2fe2e6e14f16"  # nh·∫≠p key c·ªßa b·∫°n
CSV_FILE = "Data.csv"
REVIEWS_FILE = "reviews.json"


def get_google_reviews(data_id):
    """
    G·ªçi SerpAPI ƒë·ªÉ l·∫•y review th·∫≠t t·ª´ Google Maps theo data_id
    """
    if not SERP_API_KEY:
        print("‚ö†Ô∏è Ch∆∞a c√≥ SERP_API_KEY. H√£y ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c s·ª≠a trong code.")
        return []

    url = "https://serpapi.com/search.json"
    params = {
        "engine": "google_maps_reviews",
        "data_id": data_id,
        "api_key": SERP_API_KEY
    }

    try:
        res = requests.get(url, params=params, timeout=30)
        if res.status_code != 200:
            print(f"‚ö†Ô∏è L·ªói HTTP {res.status_code} cho data_id={data_id}")
            return []
        data = res.json()
    except Exception as e:
        print(f"‚ùå L·ªói khi g·ªçi API cho {data_id}: {e}")
        return []

    reviews = data.get("reviews", [])
    result = []
    for r in reviews:
        result.append({
            "user": r.get("user", {}).get("name", "·∫®n danh"),
            "avatar": r.get("user", {}).get("thumbnail", ""),
            "rating": r.get("rating", ""),
            "comment": r.get("snippet", ""),
            "date": r.get("date", "")
        })
    return result


def crawl_all_reviews():
    """
    L·∫•y review cho to√†n b·ªô qu√°n trong Data.csv
    v√† l∆∞u v√†o reviews.json
    """
    if not os.path.exists(CSV_FILE):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {CSV_FILE}")
        return

    df = pd.read_csv(CSV_FILE)
    all_reviews = {}

    if os.path.exists(REVIEWS_FILE):
        with open(REVIEWS_FILE, "r", encoding="utf-8") as f:
            try:
                all_reviews = json.load(f)
            except json.JSONDecodeError:
                all_reviews = {}

    print(f"üìä T·ªïng qu√°n trong {CSV_FILE}: {len(df)}")
    print("üöÄ B·∫Øt ƒë·∫ßu l·∫•y review...")

    for idx, row in df.iterrows():
        data_id = str(row.get("data_id", "")).strip()
        ten_quan = row.get("ten_quan", "Kh√¥ng t√™n")

        if not data_id:
            continue

        # N·∫øu ƒë√£ c√≥ review th√¨ b·ªè qua (ƒë·ªÉ ti·∫øt ki·ªám API)
        if data_id in all_reviews:
            print(f"‚è© B·ªè qua {ten_quan} (ƒë√£ c√≥ trong reviews.json)")
            continue

        reviews = get_google_reviews(data_id)
        all_reviews[data_id] = reviews

        print(f"‚úÖ {idx+1}/{len(df)} - {ten_quan}: {len(reviews)} review")

        # L∆∞u d·∫ßn ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu
        with open(REVIEWS_FILE, "w", encoding="utf-8") as f:
            json.dump(all_reviews, f, ensure_ascii=False, indent=2)

        time.sleep(5)  # ƒë·ªÉ tr√°nh v∆∞·ª£t gi·ªõi h·∫°n SerpAPI

    print(f"üéâ Ho√†n t·∫•t! D·ªØ li·ªáu l∆∞u v√†o {REVIEWS_FILE}")


if __name__ == "__main__":
    crawl_all_reviews()
