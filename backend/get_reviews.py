import pandas as pd
import requests
import json
import os
import time

# âš™ï¸ Cáº¥u hÃ¬nh
SERP_API_KEY = "a3ce5e1007e887b80f0c3114d9bd93854917de1e7caae81e7887148f233072a4"  # giá»¯ API key cá»§a báº¡n á»Ÿ Ä‘Ã¢y

CSV_FILE = "Data_with_flavor.csv"

# ThÆ° má»¥c hiá»‡n táº¡i (vd: D:\Food_map\backend)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ThÆ° má»¥c gá»‘c project (vd: D:\Food_map)
PROJECT_ROOT = os.path.dirname(BASE_DIR)

# âœ… File user_reviews.json náº±m trong thÆ° má»¥c user_management (D:\Food_map\user_management\user_reviews.json)
REVIEWS_FILE = os.path.join(PROJECT_ROOT, "user_management", "user_reviews.json")


def get_google_reviews(data_id, max_reviews=16):
    """
    Gá»i SerpAPI Ä‘á»ƒ láº¥y tá»‘i Ä‘a `max_reviews` review (tá»‘i Ä‘a 20)
    KhÃ´ng yÃªu cáº§u pháº£i cÃ³ Ä‘á»§ cÃ¡c má»©c sao.
    """
    if not SERP_API_KEY:
        print("âš ï¸ ChÆ°a cÃ³ SERP_API_KEY. HÃ£y Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng hoáº·c sá»­a trong code.")
        return []

    all_reviews = []
    page_token = None
    page = 1

    while len(all_reviews) < max_reviews:
        params = {
            "engine": "google_maps_reviews",
            "data_id": data_id,
            "api_key": SERP_API_KEY,
        }
        if page_token:
            params["next_page_token"] = page_token

        try:
            res = requests.get("https://serpapi.com/search.json", params=params, timeout=30)
            if res.status_code != 200:
                print(f"âš ï¸ Lá»—i HTTP {res.status_code} (page {page}) cho data_id={data_id}")
                break

            data = res.json()
            reviews = data.get("reviews", [])
            if not reviews:
                break

            for r in reviews:
                all_reviews.append({
                    "user": r.get("user", {}).get("name", "áº¨n danh"),
                    "avatar": r.get("user", {}).get("thumbnail", ""),
                    "rating": r.get("rating", ""),
                    "comment": r.get("snippet", ""),
                    "date": r.get("date", "")
                })
                if len(all_reviews) >= max_reviews:
                    break

            page_token = data.get("serpapi_pagination", {}).get("next_page_token")
            if not page_token:
                break

            page += 1
            time.sleep(3)

        except Exception as e:
            print(f"âŒ Lá»—i khi gá»i API cho {data_id}: {e}")
            break

    return all_reviews[:max_reviews]


def crawl_all_reviews(start_row=None, end_row=None):
    """
    Láº¥y review cho cÃ¡c quÃ¡n trong Data.csv
    CÃ³ thá»ƒ giá»›i háº¡n tá»« dÃ²ng start_row Ä‘áº¿n end_row (tÃ­nh tá»« 1, khÃ´ng tÃ­nh header).
    Náº¿u khÃ´ng truyá»n gÃ¬ thÃ¬ sáº½ cháº¡y cho toÃ n bá»™ file.
    """
    if not os.path.exists(CSV_FILE):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file {CSV_FILE}")
        return

    df = pd.read_csv(CSV_FILE)

    # ğŸ§® Chá»n khoáº£ng dÃ²ng náº¿u ngÆ°á»i dÃ¹ng truyá»n vÃ o
    # start_row / end_row Ä‘ang tÃ­nh tá»« 1 (dÃ²ng Ä‘áº§u tiÃªn sau header lÃ  1)
    if start_row is not None or end_row is not None:
        # Náº¿u khÃ´ng nháº­p start thÃ¬ máº·c Ä‘á»‹nh tá»« 1
        if start_row is None:
            start_row = 1
        # Náº¿u khÃ´ng nháº­p end thÃ¬ máº·c Ä‘á»‹nh Ä‘áº¿n háº¿t file
        if end_row is None:
            end_row = len(df)

        # Chuyá»ƒn sang index 0-based cho pandas
        start_idx = max(start_row - 1, 0)
        end_idx = min(end_row, len(df))  # end_idx trong iloc lÃ  exclusive

        df = df.iloc[start_idx:end_idx].reset_index(drop=True)
        print(f"ğŸ” Chá»‰ crawl tá»« dÃ²ng {start_row} Ä‘áº¿n dÃ²ng {end_row} (tá»•ng {len(df)} dÃ²ng).")

    all_reviews = {}

    # Táº¡o thÆ° má»¥c chá»©a file json náº¿u chÆ°a cÃ³
    os.makedirs(os.path.dirname(REVIEWS_FILE), exist_ok=True)

    if os.path.exists(REVIEWS_FILE):
        with open(REVIEWS_FILE, "r", encoding="utf-8") as f:
            try:
                all_reviews = json.load(f)
            except json.JSONDecodeError:
                all_reviews = {}

    print(f"ğŸ“Š Tá»•ng quÃ¡n sáº½ crawl: {len(df)}")
    print("ğŸš€ Báº¯t Ä‘áº§u láº¥y review...")

    for idx, row in df.iterrows():
        data_id = str(row.get("data_id", "")).strip()
        ten_quan = row.get("ten_quan", "KhÃ´ng tÃªn")

        if not data_id:
            continue

        # âœ… Náº¿u Ä‘Ã£ cÃ³ >= 16 review thÃ¬ bá» qua
        existing = all_reviews.get(data_id, [])
        if len(existing) >= 16:
            print(f"âœ… Bá» qua {ten_quan} (Ä‘Ã£ cÃ³ {len(existing)} review)")
            continue

        print(f"ğŸ” Äang láº¥y review cho {ten_quan}...")
        reviews = get_google_reviews(data_id, max_reviews=20)
        all_reviews[data_id] = reviews

        print(f"âœ… {idx + 1}/{len(df)} - {ten_quan}: {len(reviews)} review")

        # LÆ°u dáº§n Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u
        with open(REVIEWS_FILE, "w", encoding="utf-8") as f:
            json.dump(all_reviews, f, ensure_ascii=False, indent=2)

        time.sleep(5)  # trÃ¡nh vÆ°á»£t giá»›i háº¡n API

    print(f"ğŸ‰ HoÃ n táº¥t! Dá»¯ liá»‡u lÆ°u vÃ o {REVIEWS_FILE}")


if __name__ == "__main__":
    # ğŸ‘‡ Há»i ngÆ°á»i dÃ¹ng muá»‘n cháº¡y tá»« dÃ²ng nÃ o Ä‘áº¿n dÃ²ng nÃ o
    print("Nháº­p khoáº£ng dÃ²ng muá»‘n crawl trong Data.csv (tÃ­nh tá»« 1, bá» trá»‘ng náº¿u muá»‘n tá»« Ä‘áº§u / Ä‘áº¿n cuá»‘i).")
    start_input = input("DÃ²ng báº¯t Ä‘áº§u: ").strip()
    end_input = input("DÃ²ng káº¿t thÃºc: ").strip()

    start_row = int(start_input) if start_input else None
    end_row = int(end_input) if end_input else None

    crawl_all_reviews(start_row=start_row, end_row=end_row)
