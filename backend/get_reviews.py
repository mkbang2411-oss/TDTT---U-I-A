import pandas as pd
import requests
import json
import os
import time

# âš™ï¸ Cáº¥u hÃ¬nh
SERP_API_KEY = "caf590cf1799aa732de0975966415b48a4d0911ec5c336407111c0e73fc4ed9d"

CSV_FILE = "Data_with_flavor.csv"

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(BASE_DIR)
REVIEWS_FILE = os.path.join(PROJECT_ROOT, "user_management", "user_reviews.json")

# ğŸ¯ Tá»± chá»‰nh start / end ngay táº¡i Ä‘Ã¢y
START_ROW = 2457
END_ROW = 2463

# ğŸ”’ Sá»­ dá»¥ng Ä‘Ãºng 16 review (chuáº©n Option B)
MAX_REVIEWS = 16


def get_google_reviews(data_id, max_reviews=MAX_REVIEWS):
    if not SERP_API_KEY:
        print("âš ï¸ ChÆ°a cÃ³ SERP_API_KEY.")
        return []

    all_reviews = []
    page_token = None

    while len(all_reviews) < max_reviews:
        params = {
            "engine": "google_maps_reviews",
            "data_id": data_id,
            "api_key": SERP_API_KEY,
        }
        if page_token:
            params["next_page_token"] = page_token

        try:
            res = requests.get("https://serpapi.com/search.json", params=params, timeout=30)
            if res.status_code != 200:
                print(f"âš ï¸ Lá»—i HTTP {res.status_code} cho data_id={data_id}")
                break

            data = res.json()
            reviews = data.get("reviews", [])
            if not reviews:
                break

            for r in reviews:
                all_reviews.append({
                    "user": r.get("user", {}).get("name", "áº¨n danh"),
                    "avatar": r.get("user", {}).get("thumbnail", ""),
                    "rating": r.get("rating", ""),
                    "comment": r.get("snippet", ""),
                    "date": r.get("date", "")
                })
                if len(all_reviews) >= max_reviews:
                    break

            page_token = data.get("serpapi_pagination", {}).get("next_page_token")
            if not page_token:
                break

            time.sleep(2)

        except Exception as e:
            print(f"âŒ Lá»—i khi gá»i API cho {data_id}: {e}")
            break

    return all_reviews[:max_reviews]


def crawl_all_reviews(start_row=None, end_row=None):
    if not os.path.exists(CSV_FILE):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file {CSV_FILE}")
        return

    df = pd.read_csv(CSV_FILE)

    # Náº¿u cÃ³ start/end thÃ¬ cáº¯t Ä‘Ãºng hÃ ng
    if start_row is not None or end_row is not None:
        if start_row is None:
            start_row = 1
        if end_row is None:
            end_row = len(df)

        start_idx = start_row - 1
        end_idx = end_row

        df = df.iloc[start_idx:end_idx].reset_index(drop=True)
        print(f"ğŸ” Crawl tá»« dÃ²ng {start_row} Ä‘áº¿n {end_row} â€” tá»•ng {len(df)} dÃ²ng.")

    # Load file JSON cÅ©
    all_reviews = {}
    os.makedirs(os.path.dirname(REVIEWS_FILE), exist_ok=True)

    if os.path.exists(REVIEWS_FILE):
        with open(REVIEWS_FILE, "r", encoding="utf-8") as f:
            try:
                all_reviews = json.load(f)
            except json.JSONDecodeError:
                all_reviews = {}

    print(f"ğŸ“Š Tá»•ng quÃ¡n cáº§n crawl: {len(df)}")
    print("ğŸš€ Báº¯t Ä‘áº§u...")

    for idx, row in df.iterrows():
        data_id = str(row.get("data_id", "")).strip()
        ten_quan = row.get("ten_quan", "KhÃ´ng tÃªn")

        if not data_id:
            continue

        existing = all_reviews.get(data_id, [])

        # â›” Skip Ä‘Ãºng chuáº©n 16 review
        if len(existing) >= MAX_REVIEWS:
            print(f"âœ… Bá» qua {ten_quan} (Ä‘Ã£ cÃ³ {len(existing)} review)")
            continue

        print(f"ğŸ” Äang láº¥y review cho {ten_quan}...")

        reviews = get_google_reviews(data_id, max_reviews=MAX_REVIEWS)
        all_reviews[data_id] = reviews

        print(f"âœ… {idx + 1}/{len(df)} - {ten_quan}: {len(reviews)} review")

        # LÆ°u liÃªn tá»¥c Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u
        with open(REVIEWS_FILE, "w", encoding="utf-8") as f:
            json.dump(all_reviews, f, ensure_ascii=False, indent=2)

        time.sleep(4)

    print(f"ğŸ‰ HoÃ n táº¥t! Dá»¯ liá»‡u lÆ°u táº¡i {REVIEWS_FILE}")


if __name__ == "__main__":
    crawl_all_reviews(start_row=START_ROW, end_row=END_ROW)