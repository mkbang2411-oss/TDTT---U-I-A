from nudenet import NudeDetector
import os

# Kh·ªüi t·∫°o detector (ch·ªâ 1 l·∫ßn khi import)
print("üîÑ Loading NudeNet model...")
detector = NudeDetector()
print("‚úÖ NudeNet model loaded!\n")

def check_nsfw_image_local(image_file_or_path):
    """
    Ki·ªÉm tra ·∫£nh NSFW b·∫±ng NudeNet (local, offline)
    
    Args:
        image_file_or_path: File path (str) ho·∫∑c Django UploadedFile
    
    Returns:
        dict: {
            'is_safe': bool,
            'reason': str,
            'details': dict
        }
    """
    
    temp_path = None
    cleanup = False
    
    try:
        # X·ª≠ l√Ω input
        if isinstance(image_file_or_path, str):
            temp_path = image_file_or_path
            cleanup = False
        else:
            # L∆∞u t·∫°m Django UploadedFile
            import tempfile
            temp_path = tempfile.mktemp(suffix='.jpg')
            
            with open(temp_path, 'wb') as f:
                for chunk in image_file_or_path.chunks():
                    f.write(chunk)
            
            cleanup = True
        
        # üîç PH√ÅT HI·ªÜN
        detections = detector.detect(temp_path)
        
        # üö® DANH S√ÅCH CLASS NGUY HI·ªÇM (‚úÖ T√äN ƒê√öNG)
        unsafe_classes = {
            'ANUS_EXPOSED': 'n·ªôi dung nh·∫°y c·∫£m',
            'BUTTOCKS_EXPOSED': 'n·ªôi dung nh·∫°y c·∫£m',
            'FEMALE_BREAST_EXPOSED': 'n·ªôi dung 18+',          # ‚úÖ T√äN ƒê√öNG
            'FEMALE_GENITALIA_EXPOSED': 'n·ªôi dung 18+',       # ‚úÖ T√äN ƒê√öNG
            'MALE_GENITALIA_EXPOSED': 'n·ªôi dung 18+',         # ‚úÖ T√äN ƒê√öNG
            'MALE_BREAST_EXPOSED': 'n·ªôi dung nh·∫°y c·∫£m',
        }
        
        print(f"\nüîç [NUDENET DETECTION]")
        
        max_unsafe_score = 0
        max_unsafe_class = None
        
        for detection in detections:
            class_name = detection['class']
            confidence = detection['score']
            
            print(f"   {class_name}: {confidence*100:.1f}%")
            
            # T√¨m class nguy hi·ªÉm nh·∫•t
            if class_name in unsafe_classes:
                if confidence > max_unsafe_score:
                    max_unsafe_score = confidence
                    max_unsafe_class = class_name
        
        # üö® CH·∫∂N N·∫æU > 60% CONFIDENCE
        if max_unsafe_class and max_unsafe_score > 0.6:
            print(f"   ‚ùå BLOCKED: {max_unsafe_class} ({max_unsafe_score*100:.1f}%)\n")
            
            if cleanup and os.path.exists(temp_path):
                os.remove(temp_path)
            
            return {
                'is_safe': False,
                'reason': f'Ph√°t hi·ªán {unsafe_classes[max_unsafe_class]} ({max_unsafe_score*100:.1f}%)',
                'details': {
                    'class': max_unsafe_class,
                    'confidence': round(max_unsafe_score * 100, 1),
                    'all_detections': detections
                }
            }
        
        # ‚úÖ ·∫¢NH AN TO√ÄN
        print(f"   ‚úÖ Image is safe\n")
        
        if cleanup and os.path.exists(temp_path):
            os.remove(temp_path)
        
        return {
            'is_safe': True,
            'reason': 'OK',
            'details': detections
        }
        
    except Exception as e:
        print(f"‚ùå NudeNet Error: {e}")
        import traceback
        traceback.print_exc()
        
        # Cleanup n·∫øu l·ªói
        if cleanup and temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass
        
        # Fail-safe: cho qua n·∫øu l·ªói
        return {
            'is_safe': True,
            'reason': f'Error: {str(e)}',
            'details': {}
        }